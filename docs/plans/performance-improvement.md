# Performance Improvement Plan for coodie

> **Based on**: Benchmark CI run [#64603357715](https://github.com/fruch/coodie/actions/runs/22325833587/job/64603357715?pr=31#step:5:305)
> **Date**: 2026-02-23
> **Status**: Draft â€” priorities based on initial benchmark data

---

## 1. Benchmark Summary

### 1.1 Where coodie is faster (Pydantic advantage)

| Operation | coodie | cqlengine | Ratio | Notes |
|-----------|--------|-----------|-------|-------|
| Model instantiation | 1.94 Âµs | 11.6 Âµs | **0.17Ã— (6Ã— faster)** | Pydantic compiled validators |
| Model serialization | 2.01 Âµs | 4.62 Âµs | **0.43Ã— (2.3Ã— faster)** | `model_dump()` vs cqlengine internals |

### 1.2 Where coodie is slower (ORM overhead)

| Operation | coodie | cqlengine | Ratio | Severity |
|-----------|--------|-----------|-------|----------|
| `sync_table` no-op | 4,199 Âµs | 233 Âµs | **18Ã— slower** | ðŸ”´ Critical |
| Filter (secondary index) | 19.3 ms | 4.6 ms | **4.2Ã— slower** | ðŸ”´ Critical |
| Partial UPDATE | 2,359 Âµs | 559 Âµs | **4.2Ã— slower** | ðŸ”´ Critical |
| Filter + LIMIT | 3.23 ms | 1.22 ms | **2.7Ã— slower** | ðŸŸ  High |
| GET by PK | 1,441 Âµs | 675 Âµs | **2.1Ã— slower** | ðŸŸ  High |
| UPDATE IF condition (LWT) | 3.09 ms | 1.34 ms | **2.3Ã— slower** | ðŸŸ  High |
| Collection read | 1,565 Âµs | 701 Âµs | **2.2Ã— slower** | ðŸŸ  High |
| Collection roundtrip | 2.63 ms | 1.42 ms | **1.85Ã— slower** | ðŸŸ¡ Medium |
| Bulk DELETE | 2.10 ms | 1.19 ms | **1.76Ã— slower** | ðŸŸ¡ Medium |
| Single INSERT | 1,055 Âµs | 612 Âµs | **1.72Ã— slower** | ðŸŸ¡ Medium |
| Single DELETE | 1.96 ms | 1.13 ms | **1.73Ã— slower** | ðŸŸ¡ Medium |
| Collection write | 1,086 Âµs | 677 Âµs | **1.60Ã— slower** | ðŸŸ¡ Medium |
| INSERT with TTL | 1,068 Âµs | 616 Âµs | **1.73Ã— slower** | ðŸŸ¡ Medium |
| COUNT | 1.54 ms | 1.05 ms | **1.46Ã— slower** | ðŸŸ¢ Acceptable |
| INSERT IF NOT EXISTS | 1.81 ms | 1.39 ms | **1.30Ã— slower** | ðŸŸ¢ Acceptable |

---

## 2. Root Cause Analysis

### 2.1 `_rows_to_dicts()` is the dominant bottleneck

Every query goes through `CassandraDriver._rows_to_dicts()` which iterates over
each row and converts it to a `dict`. This happens even when the driver already
returns Named Tuples (the default `row_factory`).

**Cost per row**: `hasattr` check + `_asdict()` call + `dict()` wrapping.

For queries returning N rows, this is O(N) dict allocations *before* Pydantic
even starts constructing model instances. cqlengine avoids this by working
directly with the driver's native row objects.

**Impact**: Affects every read operation â€” GET, filter, collection read, count.

### 2.2 `sync_table` always executes 3+ CQL queries

On every no-op `sync_table` call, coodie:
1. Executes `CREATE TABLE IF NOT EXISTS` (even if the table exists)
2. Queries `system_schema.columns` to detect missing columns
3. Scans all columns for index creation

cqlengine caches table metadata and skips the DDL if the table is already known.
coodie has no caching â€” every `sync_table` call hits the database.

### 2.3 `build_schema()` re-runs `get_type_hints()` on every write

`Document.save()` calls `_schema()` which calls `build_schema()`. Although there's
a cache check (`__schema__`), `_find_discriminator_column()` and
`_get_discriminator_value()` are called on every `save()` operation, each calling
`get_type_hints()` which is expensive (involves `typing` module introspection).

### 2.4 No CQL query caching at the QuerySet level

Each `QuerySet.all()`, `first()`, `get()`, etc. calls `build_select()` which
constructs a new CQL string via f-string interpolation every time. The same
logical query (e.g., "get product by id") builds the same CQL string repeatedly.

### 2.5 `get_driver()` function-level import on every call

`get_driver()` is called from inside method bodies, which means Python re-evaluates
the `from coodie.drivers import get_driver` import statement on every call.
While cached by Python's import system, the import machinery lookup has non-zero
cost at high call frequency.

### 2.6 Quadratic column scan in `sync_table`

The `sync_table` method iterates over all columns twice: once to check for missing
columns (`if col.name not in existing`) and once to check for indexes
(`if col.index`). For N columns, this is 2Ã—N iterations plus an O(N) set lookup per
column.

---

## 3. Improvement Plan

### Phase 1 â€” Quick wins (estimated 30-50% improvement on reads)

#### 3.1 Optimize `_rows_to_dicts()` by type-checking once

**Current**: checks `hasattr(row, "_asdict")` for *every* row.
**Proposed**: Check the first row's type, then use the appropriate fast path
for all remaining rows.

```python
def _rows_to_dicts(result_set):
    if not result_set:
        return []
    rows = list(result_set)
    if not rows:
        return []
    sample = rows[0]
    if hasattr(sample, "_asdict"):
        return [dict(r._asdict()) for r in rows]
    elif isinstance(sample, dict):
        return rows  # already dicts â€” zero-copy!
    else:
        return [{k: v for k, v in r.__dict__.items() if not k.startswith("_")} for r in rows]
```

**Estimated impact**: ~20-30% faster reads when `dict_factory` is in use (zero-copy);
~10% faster for Named Tuple rows (no per-row `hasattr`).

#### 3.2 Cache discriminator metadata per class

Cache the discriminator column name and value as class-level attributes after first
computation:

```python
@classmethod
def _cached_disc_info(cls):
    if not hasattr(cls, "_disc_col_cache"):
        cls._disc_col_cache = _find_discriminator_column(cls)
        cls._disc_val_cache = _get_discriminator_value(cls)
    return cls._disc_col_cache, cls._disc_val_cache
```

**Estimated impact**: Eliminates `get_type_hints()` on every `save()`/`insert()` call.
~10-15% faster writes for non-polymorphic models.

#### 3.3 Move `from coodie.drivers import get_driver` to module level

Replace function-level imports of `get_driver` with a module-level import
(guarded by lazy initialization if needed to avoid circular imports).

**Estimated impact**: Marginal per-call (~1-5%), but compounds across all operations.

### Phase 2 â€” `sync_table` optimization (target: â‰¤ 2Ã— cqlengine)

#### 3.4 Add table metadata cache to `CassandraDriver`

```python
class CassandraDriver:
    def __init__(self, ...):
        ...
        self._known_tables: set[str] = set()

    def sync_table(self, table, keyspace, cols, ...):
        cache_key = f"{keyspace}.{table}"
        if cache_key in self._known_tables:
            return  # table already synced this session
        # ... existing DDL logic ...
        self._known_tables.add(cache_key)
```

**Estimated impact**: Reduces `sync_table` no-op from 4,199 Âµs to ~0 Âµs (cache hit).
This alone closes the 18Ã— gap.

#### 3.5 Use `CREATE TABLE IF NOT EXISTS` result to skip column introspection

If the `CREATE TABLE` succeeds (table was just created), skip the
`_get_existing_columns()` query since all columns are known to be present.

**Estimated impact**: Reduces first-run `sync_table` from 3 queries to 1.

### Phase 3 â€” Query path optimization (target: â‰¤ 1.5Ã— cqlengine for reads)

#### 3.6 Build CQL for `model_dump()` directly, skipping intermediate dict

For `save()`/`insert()`, the current flow is:
```
model â†’ model_dump() â†’ dict â†’ build_insert(dict) â†’ CQL string
```

Optimize to:
```
model â†’ build_insert_from_model(model) â†’ CQL string
```

This avoids creating an intermediate dict for every write.

**Estimated impact**: ~15% faster writes.

#### 3.7 Construct Pydantic models from driver rows without intermediate dict

For reads, the current flow is:
```
driver rows â†’ _rows_to_dicts() â†’ list[dict] â†’ [Model(**dict) for dict] â†’ list[Model]
```

Optimize by passing `model_validate()` directly:
```
driver rows â†’ [Model.model_validate(row) for row] â†’ list[Model]
```

Pydantic's `model_validate()` can accept Named Tuples and mappings directly.

**Estimated impact**: Eliminates dict allocation per row. ~20-30% faster reads.

#### 3.8 Cache CQL query strings in QuerySet

For the common case of `Model.objects.filter(id=uuid).get()`, the CQL query
string is always `SELECT ... FROM ks.table WHERE "id" = ?`. Cache this:

```python
class QuerySet:
    _cql_cache: ClassVar[dict[tuple, tuple[str, list]]] = {}
```

**Estimated impact**: Eliminates f-string CQL construction on repeat queries.
~5-10% faster for repeated query patterns.

### Phase 4 â€” Filter path optimization (target: â‰¤ 2Ã— for filtered queries)

#### 3.9 Optimize `parse_filter_kwargs()` string splitting

The current filter parsing splits kwargs on `__` to detect operators. This involves
string manipulation for every filter keyword argument. Pre-compile common patterns.

#### 3.10 Reduce `QuerySet._clone()` overhead

Every chained call (`.filter().limit().allow_filtering()`) creates a new `QuerySet`
instance. Each clone copies all parameters. Consider using a builder pattern that
mutates in place until a terminal method is called.

**Estimated impact**: ~10-15% faster for complex chained queries.

### Phase 5 â€” Async optimization

#### 3.11 Eliminate `run_in_executor` for CassandraDriver async path

The current async implementation uses `run_in_executor` to delegate to the sync
`execute()`. This adds thread-pool overhead. Instead, use the cassandra-driver's
native async `execute_async()` with proper callback handling.

**Estimated impact**: ~20-40% faster async operations (eliminates thread pool hop).

---

## 4. Priority Matrix

| Phase | Task | Effort | Impact | Priority |
|-------|------|--------|--------|----------|
| 1 | 3.1 Optimize `_rows_to_dicts()` | Small | High | **P0** |
| 1 | 3.2 Cache discriminator metadata | Small | Medium | **P0** |
| 2 | 3.4 Table metadata cache | Small | Critical | **P0** |
| 1 | 3.3 Module-level `get_driver` import | Small | Low | P1 |
| 2 | 3.5 Skip column introspection on create | Medium | Medium | P1 |
| 3 | 3.7 Skip intermediate dict on reads | Medium | High | **P1** |
| 3 | 3.6 Skip intermediate dict on writes | Medium | Medium | P1 |
| 3 | 3.8 CQL query string cache | Medium | Low | P2 |
| 4 | 3.10 Reduce `_clone()` overhead | Medium | Medium | P2 |
| 4 | 3.9 Pre-compile filter patterns | Small | Low | P2 |
| 5 | 3.11 Native async for CassandraDriver | Large | High | P2 |

---

## 5. Success Criteria

After implementing P0 and P1 items:

| Metric | Current | Target |
|--------|---------|--------|
| Single INSERT latency | 1.72Ã— cqlengine | â‰¤ 1.3Ã— |
| GET by PK latency | 2.13Ã— cqlengine | â‰¤ 1.5Ã— |
| Filter + LIMIT latency | 2.66Ã— cqlengine | â‰¤ 1.8Ã— |
| `sync_table` no-op | 18Ã— cqlengine | â‰¤ 1.5Ã— |
| Partial UPDATE | 4.22Ã— cqlengine | â‰¤ 2Ã— |
| Model instantiation | 0.17Ã— cqlengine | â‰¤ 0.2Ã— (maintain advantage) |
| Model serialization | 0.43Ã— cqlengine | â‰¤ 0.5Ã— (maintain advantage) |

---

## 6. Measurement

All improvements must be validated with the benchmark suite:

```bash
# Save current baseline
pytest benchmarks/ --benchmark-enable --benchmark-save=pre-optimization

# After changes
pytest benchmarks/ --benchmark-enable --benchmark-compare=0001_pre-optimization
```

Regressions in any benchmark > 5% must be investigated before merging.

---

## 7. Notes

- coodie's Pydantic-based model system is **already 6Ã— faster** than cqlengine for
  pure Python model construction. The overhead is in the ORM â†” driver interface.
- The biggest single improvement is task 3.4 (table cache) â€” it eliminates the
  **18Ã— overhead** on `sync_table` with minimal code change.
- Tasks 3.1 and 3.7 together can significantly reduce read latency by eliminating
  unnecessary dict conversions.
- The filter-secondary-index benchmark (4.2Ã— slower) likely includes Pydantic model
  construction overhead for multiple rows â€” task 3.7 directly addresses this.
